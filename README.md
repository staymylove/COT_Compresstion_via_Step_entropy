# COT Compression via Step Entropy

Our paper: Compressing Chain-of-Thought in LLMs via Step Entropy


# Environment
To set up the environment, run the following commands:

```bash
conda create -n cot_com python==3.10
conda activate cot_com
bash requirement.sh
```


# COT Compression 
```bash

```


# Two-Stage Training

```bash

```


# Citation

```
@misc{li2025compressingchainofthoughtllmsstep,
      title={Compressing Chain-of-Thought in LLMs via Step Entropy}, 
      author={Zeju Li and Jianyuan Zhong and Ziyang Zheng and Xiangyu Wen and Zhijian Xu and Yingying Cheng and Fan Zhang and Qiang Xu},
      year={2025},
      eprint={2508.03346},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.03346}, 
}
```
